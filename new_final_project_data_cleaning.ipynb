{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c9191-4b58-40b1-9046-89429ab7552a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder.appName(\"DataCleaning\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bab5cf-ae4e-4afc-98a3-460b70f86970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "tripdata_df = spark.read.parquet(\"gs://msca-bdp-student-gcs/Group_8/tripdata\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc24ac-78b5-4d92-a680-0ceea2fbc60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display schema\n",
    "tripdata_df.printSchema()\n",
    "\n",
    "# Display first 10 rows\n",
    "tripdata_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d07e05-b59c-45a9-b257-e54f45d44b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unnecessary columns\n",
    "columns_to_drop = [\"dispatching_base_num\", \"originating_base_num\"]\n",
    "tripdata_df = tripdata_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de03f68-c016-47dc-ae0d-760caf82e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print percentage of missing values for all columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "for column in tripdata_df.columns:\n",
    "    percent_missing = 100 * (1 - (tripdata_df.select(column).dropna().count() / tripdata_df.select(column).count()))\n",
    "    print(f\"Percent missing in '{column}': {percent_missing}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e1759-a73b-4d72-8ff5-d34c648c5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with lots of missing data\n",
    "columns_to_drop = [\"on_scene_datetime\", \"airport_fee\", \"wav_match_flag\"]\n",
    "tripdata_df = tripdata_df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd34c1-4fe9-4bde-8ba9-54b575e41515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NA values from critical columns\n",
    "critical_columns = [\"wav_request_flag\"]\n",
    "tripdata_df = tripdata_df.dropna(subset=critical_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086bfce9-4a34-49a7-891c-b4271232b19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop remaining rows with any NA values\n",
    "tripdata_df = tripdata_df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f39151-f6b9-4915-8933-4757beef55c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for 'request_datetime'\n",
    "from pyspark.sql.functions import date_format, dayofweek, month, hour, when\n",
    "\n",
    "# Function to categorize time of the day\n",
    "def categorize_time_of_day(df, timestamp_column):\n",
    "    return df.withColumn(\n",
    "        \"request_time_of_day\",\n",
    "        when(date_format(timestamp_column, \"HH\").cast(\"int\").between(5, 11), \"morning\")\n",
    "        .when(date_format(timestamp_column, \"HH\").cast(\"int\").between(12, 16), \"afternoon\")\n",
    "        .when(date_format(timestamp_column, \"HH\").cast(\"int\").between(17, 20), \"evening\")\n",
    "        .otherwise(\"night\")\n",
    "    )\n",
    "\n",
    "# Apply the categorize_time_of_day function\n",
    "tripdata_df = categorize_time_of_day(tripdata_df, \"request_datetime\")\n",
    "\n",
    "# Add day of the week and month columns\n",
    "tripdata_df = tripdata_df.withColumn(\"request_hour\", hour(\"request_datetime\")) \\\n",
    "                         .withColumn(\"request_day_of_week\", dayofweek(\"request_datetime\")) \\\n",
    "                         .withColumn(\"request_month\", month(\"request_datetime\"))\n",
    "\n",
    "# Show the result\n",
    "tripdata_df.select(\"request_datetime\", \"request_time_of_day\", \"request_hour\", \"request_day_of_week\", \"request_month\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e49206-75bb-458b-bf2e-6548b7f17a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering for 'pickup_datetime'\n",
    "def categorize_time_of_day_pickup(df, timestamp_column):\n",
    "    return df.withColumn(\n",
    "        \"pickup_time_of_day\",\n",
    "        when(date_format(timestamp_column, \"HH\").cast(\"int\").between(5, 11), \"morning\")\n",
    "        .when(date_format(timestamp_column, \"HH\").cast(\"int\").between(12, 16), \"afternoon\")\n",
    "        .when(date_format(timestamp_column, \"HH\").cast(\"int\").between(17, 20), \"evening\")\n",
    "        .otherwise(\"night\")\n",
    "    )\n",
    "\n",
    "# Apply the categorize_time_of_day_pickup function\n",
    "tripdata_df = categorize_time_of_day_pickup(tripdata_df, \"pickup_datetime\")\n",
    "\n",
    "# Add day of the week and month columns for pickup_datetime\n",
    "tripdata_df = tripdata_df.withColumn(\"pickup_hour\", hour(\"pickup_datetime\")) \\\n",
    "                         .withColumn(\"pickup_day_of_week\", dayofweek(\"pickup_datetime\")) \\\n",
    "                         .withColumn(\"pickup_month\", month(\"pickup_datetime\"))\n",
    "\n",
    "# Show the result\n",
    "tripdata_df.select(\"pickup_datetime\", \"pickup_time_of_day\", \"pickup_hour\", \"pickup_day_of_week\", \"pickup_month\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6877b783-95ba-4a40-8c4c-db55233d72a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Export\n",
    "# Define the path\n",
    "output_path = \"gs://msca-bdp-student-gcs/Group_8/tripdata_cleaned\"\n",
    "\n",
    "# Write the DataFrame to the specified GCS bucket in Parquet format\n",
    "tripdata_df.write.mode(\"overwrite\").parquet(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
